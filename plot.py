import pandas as pdimport numpy as npfrom matplotlib import pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.metrics import mean_squared_error, r2_scorefrom sklearn.preprocessing import LabelEncoder, MinMaxScalerfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegression# Правки# 1) Комментарии все на русский язык# 2) Удобное скачивание используемых пайтон-библиотек (requirements.txt)# 3) Возможно стоило всё сделать в Jupyter Notebook# 4)  Добавить комментарии в пайтон стиле о входных аргументах функции# 5) Jupyter notebook (так как сохраняет графики)# 6) Seaborn, plotly для лучшей визуализации# 7) sumpy + jupyter notebook# 8) удалить из гита всё кроме скриптов, README.md и, возможно, датасета ( добавить в гитигнор)# 9) заполни файлик README.mddf = pd.read_csv('Student_Performance.csv')# Change 'Yes' and 'No' in data -> '1' and '0'encoder = LabelEncoder()df["Extracurricular Activities"] = encoder.fit_transform(df['Extracurricular Activities'])#FeaturesX = df.drop(columns = "Performance Index")#Targety = df['Performance Index']#Split dataX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# Standartisation datascaler = StandardScaler()X_train_scaled = scaler.fit_transform(X_train)X_test_scaled = scaler.transform(X_test)# Add intercept term (column of ones)X_train_scaled = np.hstack([np.ones((X_train.shape[0], 1)), X_train_scaled])X_test_scaled = np.hstack([np.ones((X_test.shape[0], 1)), X_test_scaled])# Mean Square Errordef compute_cost(X, y, theta):    n = len(y)    predictions = X.dot(theta)    cost = (1/(n)) * np.sum(np.square(predictions - y))    return cost# Gradient Descent main functiondef gradient_descent(X, y, theta, learning_rate=0.2, iterations=1000):    """Perform gradient descent to learn theta"""    n = len(y)    CostHistory = np.zeros(iterations)    for i in range(iterations):        predictions = X.dot(theta)        errors = predictions - y        #вычисление градиента функции        gradient = (1 / n) * X.T.dot(errors)        theta -= learning_rate * gradient        CostHistory[i] = compute_cost(X, y, theta)        # Early stopping if improvement is minimal        if i > 0 and abs(CostHistory[i] - CostHistory[i - 1]) < 1e-12:            print(f"Early stopping at iteration {i}")            CostHistory = CostHistory[:i]            break        #add Nesterov Accelerated Fradient (NAG)    return theta, CostHistory#Initialize parameter THETAtheta = np.zeros(X_train_scaled.shape[1])#Gradient Descent launchtheta , CostHistory = gradient_descent(X_train_scaled, y_train, theta, learning_rate = 0.0001, iterations = 50000)#outputprint("Optimized parameter is: ", theta[1:], '\n')print("Free parameter is: ", theta[0], '\n')print("Cost History: ",CostHistory,'\n')model = LinearRegression()model.fit(X_train, y_train)print("Коэффициенты sklearn:", model.intercept_, model.coef_)#Write resultes in filewith open('resultes' , 'a') as file:    file.write(str(theta))    file.write('\n')# Grafics# Make predictionsy_pred_train = X_train_scaled.dot(theta)y_pred_test = X_test_scaled.dot(theta)# Calculate metricstrain_mse = mean_squared_error(y_train, y_pred_train)test_mse = mean_squared_error(y_test, y_pred_test)train_r2 = r2_score(y_train, y_pred_train)test_r2 = r2_score(y_test, y_pred_test)print(f"Training MSE: {train_mse:.2f}, R²: {train_r2:.2f}")print(f"Test MSE: {test_mse:.2f}, R²: {test_r2:.2f}")#  X @ coef_gradient = Y_gradient -> compare with Y_true (mse)#  X @ coef_scikit = Y_scikit -> compare with Y_true (mse)print(np.min(CostHistory), test_mse)# Plot cost historyplt.figure(figsize=(10, 6))plt.plot(CostHistory, label = 'gradient')# plt.plot( test_mse, label = 'scikit')plt.xlabel('Iterations')plt.ylabel('Cost')plt.legend()plt.title('Gradient Descent: Cost Reduction Over Iterations')plt.show()# Feature importance analysisfeature_names = ['Intercept'] + list(X.columns)for name, coef in zip(feature_names, theta):    print(f"{name}: {coef:.4f}")